{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549544b7",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "The Simple Linear Regression (SLR) model describes a linear connection between a predictor variable (ùë•) and an outcome variable (ùë¶). In SLR, y is created via a linear equation with an intercept, slope, and error term. Here's a breakdown of every component:\n",
    "\n",
    "The predictor variable (x) is the independent variable used to predict outcomes. In our concept, x can represent a set of fixed or randomly generated values.\n",
    "\n",
    "Intercept (Œ≤ 0) is a constant term that represents the value of y when x = 0. This is the point when the line crosses the y-axis.\n",
    "\n",
    "Slope (Œ≤ 1): This coefficient shows the rate of change of y in relation to x. Each unit increase in ùë• x results in an average change in y of Œ≤ 1 units.\n",
    "\n",
    "The Error Term (œµ) represents random fluctuation in the linear relationship between x and y. It is assumed to be regularly distributed with a mean of zero and a given standard deviation ùúé, represented as œµ‚àºN(0,œÉ).\n",
    "\n",
    "The model equation then becomes:\n",
    "\n",
    "ùë¶ = ùõΩ 0 + ùõΩ 1 ùë• + ùúñ  where  y values are \"samples\" selected from the distribution of this equation due to the randomness imposed by œµ. The linear relationship's features are determined by the values of ùõΩ(0), ùõΩ(1), and ùúé.\n",
    "\n",
    "Here's an example Python code that generates x, y, and errors using numpy and scipy.stats. We'll also plot the data alongside the theoretical regression line.\n",
    "\n",
    "import numpy as np import scipy.stats as stats import plotly.graph_objects as go\n",
    "\n",
    "Parameters for the SLR model\n",
    "n = 50 # Number of data points beta0 = 2 # Intercept beta1 = 3 # Slope sigma = 1 # Standard deviation of the error term\n",
    "\n",
    "Generate predictor variable x (uniformly distributed)\n",
    "x = stats.uniform.rvs(0, 10, size=n)\n",
    "\n",
    "Generate errors (normally distributed)\n",
    "errors = stats.norm.rvs(0, sigma, size=n)\n",
    "\n",
    "Calculate outcome variable y based on the SLR equation\n",
    "y = beta0 + beta1 * x + errors\n",
    "\n",
    "Plotting the data and theoretical regression line\n",
    "fig = go.Figure()\n",
    "\n",
    "Scatter plot of the generated data points\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Simulated Data'))\n",
    "\n",
    "Plot the theoretical line without errors (y = beta0 + beta1 * x)\n",
    "x_line = np.linspace(min(x), max(x), 100) y_line = beta0 + beta1 * x_line fig.add_trace(go.Scatter(x=x_line, y=y_line, mode='lines', name='Theoretical Line'))\n",
    "\n",
    "Update layout\n",
    "fig.update_layout(title=\"Simple Linear Regression: Simulated Data and Theoretical Line\", xaxis_title=\"Predictor (x)\", yaxis_title=\"Outcome (y)\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "Predictor x follows a uniform distribution and can be considered fixed. Errors are sampled from a normal distribution with a mean of zero and a standard deviation of ùúé. The SLR equation is used to calculate outcome y values, which include ùõΩ0, ùõΩ1, ùë•, and the error term. The plot contains: A scatter plot of simulated data points (x,y). The theoretical regression line, ùë¶ = ùõΩ 0 + ùõΩ 1 ùë• , demonstrates the linear trend without noise. This code visually and statistically depicts the structure of an SLR model, with data points spread across the theoretical line due to the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b82b6b",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "\n",
    "Code:\n",
    "\n",
    "import numpy as np import pandas as pd import scipy.stats as stats import plotly.express as px import statsmodels.formula.api as smf\n",
    "\n",
    "Step 1: Simulate data using the theoretical SLR model\n",
    "Parameters for the SLR model\n",
    "n = 50 # Number of data points beta0 = 2 # Intercept beta1 = 3 # Slope sigma = 1 # Standard deviation of the error term\n",
    "\n",
    "Generate predictor variable x (uniformly distributed)\n",
    "x = stats.uniform.rvs(0, 10, size=n)\n",
    "\n",
    "Generate errors (normally distributed)\n",
    "errors = stats.norm.rvs(0, sigma, size=n)\n",
    "\n",
    "Calculate outcome variable Y based on the SLR equation\n",
    "Y = beta0 + beta1 * x + errors\n",
    "\n",
    "Step 2: Combine x and Y into a DataFrame for modeling\n",
    "df = pd.DataFrame({'x': x, 'Y': Y})\n",
    "\n",
    "Explanation:\n",
    "statsmodels.formula.api as smf is used for statistical modeling.\n",
    "It provides the ols (ordinary least squares) function for linear regression.\n",
    "Step 3: Specify and fit the model using statsmodels\n",
    "What the following two steps do:\n",
    "- model_data_specification sets up the formula for the regression model, specifying Y as the outcome and x as the predictor.\n",
    "- fitted_model fits the model to the data, estimating the regression coefficients.\n",
    "model_data_specification = smf.ols(\"Y ~ x\", data=df) fitted_model = model_data_specification.fit()\n",
    "\n",
    "Step 4: Obtain model summaries\n",
    "What each provides:\n",
    "print(fitted_model.summary())\n",
    "\n",
    "fitted_model.summary() gives a full summary of the regression results,\n",
    "including coefficients, standard errors, R-squared, and diagnostic information.\n",
    "print(fitted_model.summary().tables[1])\n",
    "\n",
    "fitted_model.summary().tables[1] specifically shows the table of regression coefficients\n",
    "(Intercept and x), their standard errors, t-values, and p-values.\n",
    "print(fitted_model.params)\n",
    "\n",
    "fitted_model.params returns the estimated parameters (intercept and slope).\n",
    "print(fitted_model.params.values)\n",
    "\n",
    "fitted_model.params.values is a NumPy array of the parameter values, which can be accessed without labels.\n",
    "print(fitted_model.rsquared)\n",
    "\n",
    "fitted_model.rsquared gives the R-squared value, indicating the proportion of variance in Y explained by x.\n",
    "Step 5: Visualization\n",
    "To visualize the data and fitted regression line, we add two things:\n",
    "- df['Data'] = 'Data': This labels the data points, so they appear as one color and group in the legend.\n",
    "- The trendline='ols' parameter in px.scatter automatically fits and shows a regression line.\n",
    "df['Data'] = 'Data' # Label for legend fig = px.scatter(df, x='x', y='Y', color='Data', trendline='ols', title='Y vs. x')\n",
    "\n",
    "Alternatively, add the fitted regression line manually:\n",
    "fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues, line=dict(color='blue'), name=\"trendline='ols'\")\n",
    "\n",
    "fig.show(renderer=\"png\") # For GitHub and MarkUs submissions\n",
    "\n",
    "Statistical Modelling Library:\n",
    "\n",
    "Statsmodels.formula.api, abbreviated as smf, is a statistical modelling package that offers tools for building and fitting models such as ordinary least squares regression. Model Specifications and Fitting:\n",
    "\n",
    "The model_data_specification = smf.ols(\"Y ~ x\", data=df) specifies the model formula with ùëå as the dependent variable and ùë• as the independent variable. Fitted_model equals model_data_specification.fit(): Fits the model to the data by estimating parameters (slope and intercept) with OLS.\n",
    "\n",
    "Model Summary:\n",
    "\n",
    "fitted_model.summary(): Returns a thorough summary, which includes coefficient estimates, statistical significance, R-squared, and other diagnostic data. fitted_model.summary().tables[1]: Displays the individual regression coefficient table, which is handy for quickly validating parameter estimates and p-values. fitted_model.params: Returns a labelled Series containing the estimated model parameters (intercept and slope). fitted_model.params.values: Returns the supplied values as a NumPy array without labels. Fitted_model.rsquared displays the R-squared value, which indicates how much of the variance in Y is explained by X. Plotting using Trendlines:\n",
    "\n",
    "trendline='ols': In px.scatter, this automatically fits and overlays an OLS regression line on the scatter plot of data points. Adding fitted_model.fittedvalues manually plots the model's fitted values to visually confirm the trendline.\n",
    "\n",
    "This approach demonstrates the regression line's fit to data and its effectiveness in explaining the link between x and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe0c9c",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Code: import numpy as np import pandas as pd import scipy.stats as stats import plotly.express as px import statsmodels.formula.api as smf\n",
    "\n",
    "Step 1: Simulate data using the theoretical SLR model\n",
    "Parameters for the SLR model\n",
    "n = 50 # Number of data points beta0 = 2 # Intercept beta1 = 3 # Slope sigma = 1 # Standard deviation of the error term\n",
    "\n",
    "Generate predictor variable x (uniformly distributed)\n",
    "x = stats.uniform.rvs(0, 10, size=n)\n",
    "\n",
    "Generate errors (normally distributed)\n",
    "errors = stats.norm.rvs(0, sigma, size=n)\n",
    "\n",
    "Calculate outcome variable Y based on the SLR equation\n",
    "Y = beta0 + beta1 * x + errors\n",
    "\n",
    "Combine x and Y into a DataFrame for modeling\n",
    "df = pd.DataFrame({'x': x, 'Y': Y})\n",
    "\n",
    "Specify and fit the model using statsmodels\n",
    "model_data_specification = smf.ols(\"Y ~ x\", data=df) fitted_model = model_data_specification.fit()\n",
    "\n",
    "Visualization\n",
    "df['Data'] = 'Data' # Label for legend fig = px.scatter(df, x='x', y='Y', color='Data', trendline='ols', title='Y vs. x')\n",
    "\n",
    "Add fitted regression line manually for more control\n",
    "fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues, line=dict(color='blue'), name=\"trendline='ols'\")\n",
    "\n",
    "Adding the theoretical line (from the known parameters) to the figure\n",
    "x_range = np.array([df['x'].min(), df['x'].max()]) y_line = beta0 + beta1 * x_range fig.add_scatter(x=x_range, y=y_line, mode='lines', name=str(beta0) + ' + ' + str(beta1) + ' * x', line=dict(dash='dot', color='orange'))\n",
    "\n",
    "fig.show(renderer=\"png\") # For GitHub and MarkUs submissions\n",
    "\n",
    "Explanation of the two lines and their purpose: Theoretical Line (orange, dashed): This line illustrates the \"true\" linear relationship given by our provided parameters: ùõΩ 0 = 2 Œ≤ 0=2 and ùõΩ 1 = 3 Œ≤ 1=3. It represents where the data would be if there was no random fluctuation or errors. This line is fixed and does not vary among samples because it is based solely on the pre-set model parameters.\n",
    "\n",
    "Fitted Line (blue, solid): This line is the outcome of applying an Ordinary Least Squares (OLS) regression model to the simulated data. Because the data contains random mistakes, the fitted line differs slightly between samples. This line reflects our best estimate of the relationship between x and Y for the given dataset.\n",
    "\n",
    "The goal of comparing these lines is to illustrate the effect of random sampling variance by charting both lines. Because the fitted line is based on a different sample of data points, it will vary slightly each time we employ random errors to generate a new dataset. However, because it represents the actual underlying model devoid of noise, the theoretical line stays constant. Because of the random mistakes in each sample, it is possible to show how far or near the fitted model can deviate from the theoretical relationship by repeatedly creating new samples and comparing the lines. In regression analysis, this procedure aids in comprehending the idea of sample variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d638baf",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "Within the theoretical framework:\n",
    "\n",
    "Y i = Œ≤ 0 + Œ≤ 1 x i al + œµ i.\n",
    "\n",
    " \n",
    "The value of the dependent variable is represented by Y i.\n",
    "The relationship between Y and x has a real slope of ùõΩ 1 Œ≤ 1 and a true intercept of ùõΩ 0 Œ≤ 0.\n",
    "The predictor variable is x i, and the random error term is represented by ùúñùëñ i, which is assumed to have a normal distribution with mean 0 and standard deviation œÉ.\n",
    "We employ estimated coefficients in the fitted model:\n",
    "\n",
    "ùëå^ùëñ = ùõΩ^0 + ùõΩ^1 ùë•ùëñ \n",
    "The sample-based anticipated or fitted value of Y is denoted by i.\n",
    "ùõΩ^0 \n",
    "\n",
    "  \n",
    "Between 0 and ùõΩ^1 \n",
    "\n",
    "  \n",
    "OLS (Ordinary Least Squares) is used to obtain the observed data, from which the estimates of Œ≤ 0 and¬†Œ≤ 1 are derived.\n",
    "\n",
    "\n",
    "Fitted_model.fittedvalues is obtained.\n",
    "Calculate your coefficients:\n",
    "\n",
    "Fitted_model = model_data_specification.fit() computes the values of ùõΩ^0 Œ≤^ when we fit the model.\n",
    "\n",
    "  \n",
    "Between 0 and ùõΩ^1\n",
    "\n",
    "  \n",
    "1 to reduce the sum of squared discrepancies between the fitted Y^ and the observed Y.\n",
    "The functions fitted_model.params and fitted_model.summary() provide access to these calculated coefficients.The estimations and their statistical characteristics are also displayed in tables.\n",
    "\n",
    "Determine the Fitted Values:\n",
    "\n",
    "Each x i from the dataset is entered into the fitted model equation to calculate the fitted values, fitted_model.fittedvalues:\n",
    "ùëå^ùëñ = ùõΩ^0 + ùõΩ^1 ùë•ùëñ \n",
    "\n",
    " \n",
    "For every observed x i in the dataset, this generates a set of predicted values.\n",
    "\n",
    "\n",
    "Differentiating Fitted Values from Theoretical Values\n",
    "\n",
    "Using the fixed (true) parameters ùõΩ0, the theoretical model produces Y values.\n",
    "\n",
    "and ùõΩ 1\n",
    "\n",
    "in addition to random error ùúñùëñ.\n",
    "\n",
    "The fitted model, on the other hand, produces predictions ùëå^ùëñ\n",
    "\n",
    "utilizing the calculated coefficients ùõΩ^0 and ùõΩ^1 obtained from the information. Since there is no random error term included in these predictions, they fall exactly on the fitted line with no scatter.\n",
    "\n",
    "This question aims to demonstrate the distinction between using a fitted model based on sample estimates to forecast values and creating data using a theoretical model (with fixed parameters and random error). Understanding that fitted values are unique to the observed sample and represent estimated associations rather than the \"true\" underlying relationship as stated by the theoretical model requires an awareness of this discrepancy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6439a",
   "metadata": {},
   "source": [
    "5.\n",
    "\n",
    "OLS Description and the Function of Squared Residuals\n",
    "Why Residuals Are Important\n",
    "\n",
    "The difference between the actual observed value, Y i, and the anticipated value, Y^, is the residual for each data point.\n",
    "  \n",
    "on the line that is fitted. It can be expressed mathematically as follows: ùëíùëñ = ùëåùëñ ‚àíùëå^ùëñ\n",
    "  \n",
    "These residuals show the model's prediction inaccuracy for each observation and show the vertical separation of each data point from the fitted line.\n",
    "\n",
    "Reasons for Reducing the Sum of Squares:\n",
    "\n",
    "Negative and positive residuals may cancel each other out if the residuals are simply minimized without being squared, producing an inaccurate evaluation of \"fit.\"\n",
    "Finding the line that minimizes the overall departure of the data points from the line is made possible by OLS, which squares each residual to guarantee that each one contributes positively to the total error.\n",
    "\n",
    "The Fitted Line and OLS:\n",
    "\n",
    "The line that best depicts the linear connection between x and y in the observed data is chosen by the OLS method, which minimizes the sum of squared residuals.\n",
    "This fitted line can be thought of as our \"best guess\" of the linear trend based on the sample data and is an estimate of the underlying connection.\n",
    "\n",
    "To summarize, OLS selects the line that minimizes the overall \"distance\" (measured in squares) between the line and the observed data points. A more precise measure of fit is made possible by squaring the residuals, which also guarantees that every data point's departure from the line adds to the total error without having any cancelling effects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52498da",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "Explanation of Each Expression\n",
    "\n",
    "1. Proportion of Variation Explained by the Model\n",
    "1‚àí‚àë(Y‚àíY^)2‚àë(Y‚àíYÀâ)21 - \\frac{\\sum (Y - \\hat{Y})^2}{\\sum (Y - \\bar{Y})^2}1‚àí‚àë(Y‚àíYÀâ)2‚àë(Y‚àíY^)2‚Äã\n",
    "This expression is known as the coefficient of determination or R2R^2R2 and is interpreted as the proportion of variation in YYY that is explained by the model (i.e., the fitted values Y^\\hat{Y}Y^).\n",
    "‚àë(Y‚àíY^)2\\sum (Y - \\hat{Y})^2‚àë(Y‚àíY^)2 is the residual sum of squares (RSS), which quantifies the total error of the model‚Äôs predictions. It measures how much the model‚Äôs predictions deviate from the actual observed values.\n",
    "‚àë(Y‚àíYÀâ)2\\sum (Y - \\bar{Y})^2‚àë(Y‚àíYÀâ)2 is the total sum of squares (TSS), which represents the total variability in YYY around its mean YÀâ\\bar{Y}YÀâ.\n",
    "By taking 1‚àíRSSTSS1 - \\frac{\\text{RSS}}{\\text{TSS}}1‚àíTSSRSS‚Äã, we calculate the proportion of YYY‚Äôs variability that is explained by the model. A higher R2R^2R2 value indicates that a greater proportion of the variance in YYY is accounted for by the linear relationship with xxx.\n",
    "\n",
    "2.model_fitted_rsquared\n",
    "The R2 value is directly calculated using the same formula as previously utilizing this number, which is supplied by the model summary in statsmodels.\n",
    "\n",
    "A summary of model correctness that shows how well the model fits the data is provided by fitted_model.rsquared. The majority of the variability in Y may be explained by the model if fitted_model.rsquared is near 1.\n",
    "\n",
    "\n",
    "3. fitted_model.fittedvalues, np.corrcoef(Y)[0, 1]**2\n",
    "This formula determines the squared correlation between the model's fitted values, Y^\\hat{Y}Y^, and the observed values, YYY.\n",
    "The degree and direction of a linear link between two variables are measured by the correlation coefficient, or rrr. A measure of the percentage of variance in YYY that can be predicted from Y^\\hat{Y}Y^ is obtained by squaring it (r2); in the context of Simple Linear Regression, this is comparable to R2R^2R2.\n",
    "\n",
    "Consequently, the same information as fitted_model.rsquared is provided by np.corrcoef(Y, fitted_model.fittedvalues)[0,1]**2, which shows the percentage of variation in YYY that the model can account for.\n",
    "\n",
    "4. np.corrcoef(Y, x)[0, 1]**2\n",
    "This expression calculates the squared correlation between Y and the predictor x, capturing the proportion of the variation in Y that can be explained by its linear relationship with x.\n",
    "In Simple Linear Regression, since x is the only predictor and Y is a linear transformation of x, this squared correlation will also match R^2. Thus, np.corrcoef(Y, x)[0, 1]**2 serves as another way to compute the model‚Äôs R^2.\n",
    "\n",
    "Together, they verify that the squared correlation between Y and x, or between Y and Y^hat, in Simple Linear Regression accurately represents the model's goodness of fit. Greater values signal that the model has more predictive potential for Y given x since it explains a greater percentage of Y's variability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7b4d5",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "\n",
    "1.The assumption of linearity\n",
    "\n",
    "A linear relationship between the predictor variable (fertilizer amount) and the outcome variable (crop yield) is assumed by simple linear regression. Observation: The scatter plot of crop output vs fertilizer amount suggests that there may be more than one way to interpret the link between fertilizer amount and yield. Crop output appears to climb gradually at first, then sharply, and then flattens out at greater fertilizer levels. This pattern points to a potential non-linear relationship in which crop yield may decrease as fertilizer use increases.\n",
    "\n",
    "2 Homoscedasticity, or Constant Error Variance Simple Linear Regression makes the assumption that the residuals' (errors') variance is constant at all predictor variable levels. Remainders are not evenly distributed around zero across the range of fertilizer quantities, according to the residuals histogram. As fertilizer levels rise, there seems to be greater variance in crop output, which could indicate heteroscedasticity. In other words, increased fertilizer consumption levels may result in greater production variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502c7f1",
   "metadata": {},
   "source": [
    "8.\n",
    "\n",
    "Null Hypothesis in Simple Linear Regression: The slope, Œ≤‚ÇÅ, is the pertinent parameter of the Simple Linear Regression model that can be used to describe the null hypothesis for the statement \"no linear association (on average)\". The relationship between the waiting period and the eruption's duration is of importance to us in this instance, where:\n",
    "\n",
    "H‚ÇÄ (Null Hypothesis): The average waiting time and duration do not correlate linearly. According to the regression model, this indicates that the regression line's (Œ≤‚ÇÅ) slope is zero: H0:Œ≤1 = 0 This suggests that the eruption duration is unaffected by the waiting time, and any correlation that is seen may be the result of chance variations in the data. The alternative hypothesis, or H‚ÇÅ, states that waiting time and duration have a linear connection. The slope (Œ≤‚ÇÅ) is therefore not equal to zero: The formula is 0H1:Œ≤1‚â†0H‚ÇÅ: Œ≤‚ÇÅ = 0.\n",
    "\n",
    "Examining the Null Hypothesis Applying the Code: Using the statsmodels.formula.api library, the code you gave fits a model based on the following formula and does a simple linear regression (OLS):\n",
    "\n",
    "linear_for_specification = 'duration ~ waiting'\n",
    "\n",
    "The crucial actions that are involved are: Fit the Linear Model: This involves fitting the data to an ordinary least squares (OLS) regression. Summary Output: The p-value for the slope (Œ≤‚ÇÅ) and other information about the regression results are provided by fitted_model.summary().\n",
    "\n",
    "import seaborn as sns import statsmodels.formula.api as smf\n",
    "\n",
    "Load the Old Faithful dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "Specify the linear model\n",
    "linear_for_specification = 'duration ~ waiting'\n",
    "\n",
    "Fit the OLS model\n",
    "model = smf.ols(linear_for_specification, data=old_faithful) fitted_model = model.fit()\n",
    "\n",
    "Print the summary of the fitted model\n",
    "print(fitted_model.summary())\n",
    "\n",
    "Interpretation of the Output: The following crucial details pertinent to evaluating the hypothesis will be included in the fitted_model.summary(): Coefficient of Slope (Œ≤‚ÇÅ): The predicted waiting coefficient, or the amount that the eruption length varies with each unit increase in waiting time. The slope coefficient's p-value is: This determines if there is a significant difference between Œ≤‚ÇÅ and zero. The key to testing the null hypothesis is the p-value linked to the waiting coefficient: We reject the null hypothesis if the waiting p-value is less than 0.05, which shows that the waiting time and eruption duration have a statistically significant linear connection. The null hypothesis cannot be rejected if the p-value is higher than 0.05, indicating that there is not enough data to support a linear relationship between waiting time and eruption duration.\n",
    "\n",
    "Example of Results Interpretation: Assume we have the fitted_model output.The following is an example of what summary() contains:\n",
    "\n",
    "0.1 is the estimated slope (Œ≤‚ÇÅ), and the waiting p-value is 0.002. You would reject the null hypothesis and come to the conclusion that waiting and length have a significant linear connection because the p-value is less than 0.05. This would imply that longer eruption durations are generally linked to longer waiting times.\n",
    "\n",
    "However, if the p-value were, say, 0.15, the null hypothesis would not be rejected, indicating that there is insufficient evidence in the data to support the argument that waiting time and eruption duration have a significant linear connection. Conclusion: You may conclude that the waiting period has a statistically significant impact on the eruption's duration if you reject the null hypothesis. You would conclude that there is insufficient evidence of a linear relationship between these two variables if you are unable to reject the null hypothesis.\n",
    "\n",
    "‚Äã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cd854",
   "metadata": {},
   "source": [
    "9.\n",
    "\n",
    "To assess whether there is evidence of a relationship between duration and waiting for short wait times (i.e., those less than a specified short_wait_limit), we will proceed in the following steps:\n",
    "\n",
    "Subset the Data for Short Wait Times: We filter the dataset to only include records where the waiting time is less than the specified short_wait_limit (e.g., 62, 64, or 66).\n",
    "Fit an OLS Model: We fit a linear regression model using statsmodels to evaluate whether there is a statistically significant relationship between waiting and duration within the short wait time subset.\n",
    "Hypothesis Test for the Slope: We test the null hypothesis (H‚ÇÄ) that there is no relationship between waiting and duration (i.e., the slope Œ≤‚ÇÅ = 0).\n",
    "Characterize Evidence: We interpret the p-value for the slope to determine if there is evidence against the null hypothesis.\n",
    "Visualize the Relationship: We create a scatter plot with the data and the fitted regression line to visualize any relationship.\n",
    "For short wait times, the null hypothesis is: H‚ÇÄ (Null Hypothesis): For brief wait times, there is no linear relationship between waiting time and duration (i.e., Œ≤‚ÇÅ = 0). H‚ÇÅ (Alternative Hypothesis): For brief wait times, waiting time and length have a linear relationship (i.e., Œ≤‚ÇÅ ‚â† 0).\n",
    "\n",
    "import plotly.express as px import statsmodels.formula.api as smf\n",
    "\n",
    "Define the short wait time limit (can change to 64 or 66 for different subsets)\n",
    "short_wait_limit = 62 # Try 64 or 66 for other cases\n",
    "\n",
    "Subset the data for short wait times\n",
    "short_wait = old_faithful.waiting < short_wait_limit\n",
    "\n",
    "Fit the OLS regression model for the short wait time subset\n",
    "model_summary = smf.ols('duration ~ waiting', data=old_faithful[short_wait]).fit().summary().tables[1] print(model_summary)\n",
    "\n",
    "Create a scatter plot with a linear regression trendline for short wait times\n",
    "fig = px.scatter(old_faithful[short_wait], x='waiting', y='duration', title=\"Old Faithful Geyser Eruptions for short wait times (<\" + str(short_wait_limit) + \")\", trendline='ols')\n",
    "\n",
    "fig.show() # Use fig.show(renderer=\"png\") for all GitHub and MarkUs submissions\n",
    "\n",
    "Key Findings from the Model Summary Broken Down: Coefficients Table: The regression coefficients, standard errors, t-values, and p-values are all included in this table. In particular, we examine: Intercept (Œ≤‚ÇÄ): The regression model's estimated intercept. The coefficient that interests us is the slope for waiting (Œ≤‚ÇÅ). In the event that it deviates sufficiently from zero, the null hypothesis can be rejected. The crucial statistic for determining whether to reject the null hypothesis is the p-value for waiting; if it is less than 0.05, we reject H‚ÇÄ.\n",
    "\n",
    "Interpretation: We can conclude that there is evidence against the null hypothesis if the waiting p-value is less than 0.05. This indicates that, for brief wait durations, waiting has a statistically significant linear relationship with duration. The null hypothesis would not be rejected if the waiting p-value was higher than 0.05, indicating that there is no meaningful correlation between waiting time and duration for brief wait times.\n",
    "\n",
    "Visualization: The scatter plot, which illustrates the correlation between waiting time and duration for brief wait times, can assist you in visually examining the data. The plotting of the linear regression trendline will shed further light on the nature of the relationship.\n",
    "\n",
    "Conclusion: There is insufficient data to support a linear relationship between waiting time and duration in the setting of brief wait times, if the null hypothesis is not rejected. Rejecting the null hypothesis would imply that there is some indication of a linear relationship between the variables, even for brief wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274818c",
   "metadata": {},
   "source": [
    "10.\n",
    "\n",
    "1.Simple Linear Regression Bootstrapping:\n",
    "\n",
    "We'll use bootstrap sampling to generate several resampled datasets for the 160 lengthy wait times, fit a Simple Linear Regression model to each sample, and gather the fitted slope coefficients. See the slope coefficients' bootstrapped distribution.\n",
    "\n",
    "2.Data Simulation Using the Null Hypothesis:\n",
    "\n",
    "A straightforward linear regression model with a slope of ùõΩ1=0 (i.e., no linear relationship) can be used to simulate y values. To generate the sampling distribution, fit Simple Linear Regression models to these simulated samples and gather the slope coefficients. Examine whether 0 falls inside the bootstrapped slope coefficients' 95% confidence interval by visualizing the sampling distribution.\n",
    "\n",
    "3.In contrast to the original model:\n",
    "\n",
    "Compare the p-value from the simulated model with the p-value from the original regression (smf.ols('duration ~ waiting', data=old_faithful).fit()) and report if the 95% bootstrapped confidence interval contains 0.\n",
    "\n",
    "Code Implementation:\n",
    "\n",
    "import seaborn as sns import numpy as np import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf import plotly.express as px\n",
    "\n",
    "Load the Old Faithful dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "Subset the data to include only long wait times\n",
    "long_wait_times = old_faithful[old_faithful['waiting'] > 63]\n",
    "\n",
    "Part 1: Bootstrapping for Simple Linear Regression on Long Wait Times\n",
    "n_bootstrap = 1000 bootstrap_slopes = []\n",
    "\n",
    "Perform bootstrapping\n",
    "for _ in range(n_bootstrap): sample = long_wait_times.sample(n=len(long_wait_times), replace=True) # Resample with replacement X = sm.add_constant(sample['waiting']) # Add constant for intercept y = sample['duration'] model = sm.OLS(y, X).fit() # Fit model bootstrap_slopes.append(model.params[1]) # Collect the slope coefficient (beta1)\n",
    "\n",
    "Visualize the distribution of bootstrapped slope coefficients\n",
    "fig1 = px.histogram(bootstrap_slopes, nbins=30, title=\"Bootstrapped Sampling Distribution of Slope Coefficients\") fig1.show()\n",
    "\n",
    "Part 2: Simulating Under the Null Hypothesis\n",
    "n_simulations = 1000 simulated_slopes = []\n",
    "\n",
    "Simulate samples under the null hypothesis: Œ≤1 = 0, Œ≤0 = 1.65, œÉ = 0.37\n",
    "np.random.seed(42) # For reproducibility beta_0 = 1.65 beta_1 = 0 # Null hypothesis: no linear relationship sigma = 0.37 waiting_values = long_wait_times['waiting'].values\n",
    "\n",
    "for _ in range(n_simulations): y_simulated = beta_0 + beta_1 * waiting_values + np.random.normal(0, sigma, size=len(waiting_values)) X_sim = sm.add_constant(waiting_values) # Add constant for intercept model_sim = sm.OLS(y_simulated, X_sim).fit() # Fit model to simulated data simulated_slopes.append(model_sim.params[1]) # Collect the slope coefficient\n",
    "\n",
    "Visualize the distribution of simulated slope coefficients\n",
    "fig2 = px.histogram(simulated_slopes, nbins=30, title=\"Simulated Sampling Distribution of Slope Coefficients (Null Hypothesis)\") fig2.show()\n",
    "\n",
    "Calculate 95% confidence interval from the bootstrapped slopes\n",
    "bootstrap_ci_lower = np.percentile(bootstrap_slopes, 2.5) bootstrap_ci_upper = np.percentile(bootstrap_slopes, 97.5) print(f\"Bootstrapped 95% Confidence Interval for the Slope: ({bootstrap_ci_lower:.3f}, {bootstrap_ci_upper:.3f})\")\n",
    "\n",
    "Check if 0 is within the 95% CI\n",
    "if bootstrap_ci_lower <= 0 <= bootstrap_ci_upper: print(\"0 is contained within the 95% bootstrapped confidence interval.\") else: print(\"0 is NOT contained within the 95% bootstrapped confidence interval.\")\n",
    "\n",
    "Report the p-value from the simulated regression and the original regression\n",
    "simulated_p_values = [] for slope in simulated_slopes: # Calculate a p-value for the hypothesis test based on the null distribution t_stat = slope / (sigma / np.sqrt(len(waiting_values))) # Standard error assumed as sigma/sqrt(n) p_value = 2 * (1 - sm.stats.t.cdf(np.abs(t_stat), df=len(waiting_values)-2)) # Two-tailed p-value simulated_p_values.append(p_value)\n",
    "\n",
    "Calculate the original model's p-value for comparison\n",
    "original_model = smf.ols('duration ~ waiting', data=old_faithful).fit() original_p_value = original_model.pvalues[1] # p-value for the 'waiting' coefficient\n",
    "\n",
    "Calculate the proportion of simulated p-values less than 0.05\n",
    "simulated_p_value_proportion = np.mean(np.array(simulated_p_values) < 0.05)\n",
    "\n",
    "print(f\"Original p-value for 'waiting' in the actual model: {original_p_value:.4f}\") print(f\"Proportion of simulated p-values less than 0.05: {simulated_p_value_proportion:.4f}\")\n",
    "\n",
    "Findings: Confidence Interval: We can determine whether the true slope is zero by looking at the slope's 95% bootstrapped confidence interval. It indicates that there is no proof of a meaningful linear relationship if it contains zero. p-Value Comparison: To determine whether they are similar, the p-values from the real regression and the simulated model under the null hypothesis will be compared. This sheds light on whether the null hypothesis can be rejected based on the observed evidence.\n",
    "\n",
    "Interpretation: The null hypothesis that there is no linear relationship in the data cannot be rejected if the bootstrapped confidence interval contains 0. We can determine the power of the simulation-based hypothesis test if the percentage of simulated p-values less than 0.05 is high, as this suggests that the null hypothesis is probably rejected under the null assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa8416",
   "metadata": {},
   "source": [
    "11.\n",
    "\n",
    "The text from the picture is as follows:\n",
    "\n",
    "Using an indicator (dummy) variable for wait time length, we will specify a Simple Linear Regression model in the new method. This model introduces a categorical variable kind (with two levels: \"short\" and \"long\") to distinguish between \"short\" and \"long\" wait times.\n",
    "\n",
    "Model Details\n",
    "\n",
    "The following is the model that you are inquiring about:\n",
    "\n",
    "Yi = 1{long(ki)} + Œ≤interceptŒ≤contrast plus Œµi\n",
    "\n",
    "Location:\n",
    "\n",
    "Y i is the duration of the geyser eruption for observation i, k i is the category of wait time (either \"short\" or \"long\"), 1 {long(k i)} is an indicator variable for whether the wait time is in the \"long\" category (coded as 1 if the wait time is long and 0 if it is short), and ùõΩ intercept The projected eruption length in the case of a short wait time (reference category) is represented by the Œ≤ intercept, while the difference in eruption duration between long and short wait times is represented by the ùõΩ contrast.\n",
    "\n",
    "Significant Disparities We present a binary categorical variable (kind) in this new model specification that divides wait durations into two groups: \"short\" and \"long.\" This is in contrast to the earlier model specifications, which assumed a linear relationship between waiting time and duration without taking into account various groups (categories) and employed a continuous predictor (waiting).\n",
    "\n",
    "Previous models: We presumed a single linear trend for all data (independent of wait time length) by using continuous values of waiting as the predictor. Estimating the impact of the waiting period on the eruption's duration was the main goal.\n",
    "\n",
    "Current model: We handle waiting as a categorical variable with two levels, \"short\" and \"long,\" rather than as a continuous variable. This enables the estimation of two distinct intercepts by the model:\n",
    "\n",
    "One for the \"long\" group (modified by ùõΩ contrast), and one for the \"short\" group (the reference category).\n",
    "\n",
    "Thus, the current model allows us to test if the average duration of geyser eruptions differs between short and long wait times, without assuming a continuous relationship between waiting and duration.\n",
    "\n",
    "Code for fitting the model and reporting results:\n",
    "\n",
    "from IPython.display import display import statsmodels.formula.api as smf import plotly.express as px\n",
    "\n",
    "Fit the model with \"kind\" as a categorical variable using Treatment encoding\n",
    "model = smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful).fit()\n",
    "\n",
    "Display the model summary (showing the coefficients and p-values)\n",
    "display(model.summary().tables[1])\n",
    "\n",
    "Create a box plot to visualize the distribution of 'duration' for the two 'kind' categories\n",
    "fig = px.box(old_faithful, x='kind', y='duration', title='duration ~ kind', category_orders={'kind': ['short', 'long']}) fig.show() # USE fig.show(renderer=\"png\") FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "\n",
    "Reporting Evidence Against the Null Hypothesis: The null hypothesis for this model is:\n",
    "\n",
    "H‚ÇÄ: There is no difference in the average duration between the \"short\" and \"long\" wait time categories. H‚ÇÅ: There is a difference in the average duration between the \"short\" and \"long\" wait time categories.\n",
    "\n",
    "Visual Interpretation: The results will be supported by the box plot, which indicates that there is a discernible difference between the two groups and that the duration distribution for \"long\" wait periods is probably moved upward in comparison to the \"short\" group.\n",
    "\n",
    "In conclusion, we reject the null hypothesis that there is no difference between the \"short\" and \"long\" wait time groups if the p-value for ùõΩ comparison is less than 0.05, which denotes a statistically significant difference. This conclusion is graphically supported by the box plot, which displays the duration difference between the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713b400",
   "metadata": {},
   "source": [
    "Summary:\n",
    "Here's a summary of everything you've asked about:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Simple Linear Regression for Old Faithful Geyser**\n",
    "   You began by exploring a **Simple Linear Regression** model with the Old Faithful Geyser dataset, aiming to understand the relationship between the waiting time before an eruption (`waiting`) and the eruption duration (`duration`).\n",
    "\n",
    "   - **Null Hypothesis**: You tested whether there is no linear association between `waiting` and `duration`. This was represented as: \n",
    "     \\[\n",
    "     H_0: \\beta_1 = 0\n",
    "     \\]\n",
    "     where \\(\\beta_1\\) is the slope of the regression line.\n",
    "\n",
    "   - **Code Explanation**: \n",
    "     You used `statsmodels` to fit the linear regression model and interpret the coefficients, p-values, and confidence intervals. The model was visualized using **Plotly** with a trendline (OLS) and an optional **LOWESS** smoothed trendline.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Short vs. Long Wait Times**\n",
    "   You decided to further explore how `duration` and `waiting` relate by dividing the data into **short** and **long** wait times (with thresholds of 64, 66, and finally 68 minutes) and analyzed this new categorical variable `kind`.\n",
    "\n",
    "   - **New Model**: You shifted from treating `waiting` as continuous to using `kind` as a categorical variable, with `\"short\"` as the reference category and `\"long\"` as the contrasting category. The model is:\n",
    "     \\[\n",
    "     Y_i = \\beta_{\\text{intercept}} + 1\\{\\text{long}(k_i)\\}\\beta_{\\text{contrast}} + \\epsilon_i\n",
    "     \\]\n",
    "     where \\(\\beta_{\\text{contrast}}\\) is the difference in eruption duration between `\"long\"` and `\"short\"` wait times.\n",
    "   \n",
    "   - **Null Hypothesis**: For this new model, the null hypothesis is:\n",
    "     \\[\n",
    "     H_0: \\beta_{\\text{contrast}} = 0 \\quad (\\text{no difference between groups on average}).\n",
    "     \\]\n",
    "     You used **Treatment Encoding** to specify the model and performed hypothesis testing to check if there was a significant difference between `\"short\"` and `\"long\"` wait times. The significance was assessed by checking the p-value of the contrast coefficient (\\(\\beta_{\\text{contrast}}\\)).\n",
    "\n",
    "   - **Box Plot**: A **box plot** was created to visualize the distribution of `duration` for the two categories of `kind` (short and long), helping to confirm any significant differences in eruption durations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Bootstrapping and Simulation for Slope Coefficients**\n",
    "   Next, you explored the concept of **bootstrapping** and **simulation** to examine the stability of the slope coefficient from a Simple Linear Regression model under both real and simulated scenarios.\n",
    "\n",
    "   - **Bootstrapping**: You resampled the long wait times (n=160) with replacement, fitted Simple Linear Regression models to each bootstrap sample, and visualized the bootstrapped distribution of slope coefficients.\n",
    "   \n",
    "   - **Simulation Under the Null Hypothesis**: You simulated data under the assumption that there is **no linear association** (i.e., \\(\\beta_1 = 0\\)) by generating random `y` values with a known intercept and error term. You then collected and visualized the sampling distribution of the slope coefficient under this assumption and compared it with the observed slope from the original data.\n",
    "\n",
    "   - **Confidence Interval and p-Value**: You used the bootstrapped confidence intervals to check if 0 was contained within the interval and compared the p-value from the simulated data with the p-value from the original regression model on the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Indicator Variable Model**\n",
    "   In your final exploration, you wanted to model the **duration** of eruptions using a new indicator variable for **short** and **long** wait times based on a threshold of 68 minutes. The model specification is:\n",
    "   \\[\n",
    "   Y_i = \\beta_{\\text{intercept}} + 1\\{\\text{long}(k_i)\\}\\beta_{\\text{contrast}} + \\epsilon_i\n",
    "   \\]\n",
    "   You then fitted an **OLS model** with `kind` as a categorical variable, using **Treatment encoding** to differentiate between short and long wait times.\n",
    "\n",
    "   - **Null Hypothesis**: For the new model, the null hypothesis is that there is no difference between the two categories (short and long), i.e., \\(\\beta_{\\text{contrast}} = 0\\).\n",
    "   - **Model Interpretation**: You assessed the **p-value** for \\(\\beta_{\\text{contrast}}\\) to test whether the difference in eruption duration between the two categories was statistically significant. A **box plot** was also created to visualize the difference between the two groups.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights and Takeaways:\n",
    "\n",
    "1. **Modeling Relationships**:\n",
    "   - When `waiting` is treated as a continuous variable, you assume a single linear relationship across all data.\n",
    "   - By using an indicator variable (`kind`), you split the data into two groups, which allows you to model different intercepts for short and long wait times, and test whether there is a significant difference between them.\n",
    "\n",
    "2. **Statistical Significance**:\n",
    "   - When bootstrapping or simulating data under the null hypothesis, you compared the p-values and confidence intervals to understand if the observed effect (in the case of `waiting` or `kind`) is statistically significant.\n",
    "   - A **p-value less than 0.05** in the indicator variable model suggests a significant difference between short and long wait times.\n",
    "\n",
    "3. **Visualizations**:\n",
    "   - **Scatter plots with trendlines** and **box plots** helped you visually assess the relationships and differences in the data.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the summary of all your questions and the analyses performed based on the Old Faithful Geyser dataset, from regression modeling and hypothesis testing to bootstrapping and simulations! Let me know if you need any further clarifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fbeab",
   "metadata": {},
   "source": [
    "Link: https://chatgpt.com/share/672c0f12-98fc-800e-817b-242c9d2ded6c "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878e603",
   "metadata": {},
   "source": [
    "12.\n",
    "\n",
    "We must consider how closely the distribution of the residuals matches the theoretical normal distribution (bell-shaped curve, symmetric around zero) in order to determine which histogram best supports the hypothesis that the error components in each model are normally distributed. Let's examine each model and the information provided by the histograms.\n",
    "\n",
    "Procedures for Assessment:\n",
    "Analyze Symmetry Around zero, the histogram ought to be symmetrical. Asymmetry or skewness point to non-normality.\n",
    "The Normal Distribution Curve is compared to: As a reference, a normal distribution curve is placed on each histogram. The likelihood that the residuals are regularly distributed increases with how closely they resemble this curve.\n",
    "Seek out anomalies: Non-normality can also be shown by outliers. They could appear as spikes that are far from the distribution's centre.\n",
    "\n",
    "Model Interpretation:\n",
    "Model 1 (Slope for All Data): The normality assumption is tenable for this model if the histogram closely resembles the normal distribution curve and may display a somewhat normal distribution with some spread.\n",
    "\n",
    "Model 2 (Short Wait Data): A more asymmetrical shape may be seen in the second histogram. Large departures from the normal curve or a skewed distribution would indicate that the premise of normalcy is not supported. Here, a different bin size has been used, which can highlight the data's irregularity.\n",
    "\n",
    "In a similar vein, Model 3 (Long Wait Data) may also have a non-normal shape, either bimodal or skewed, indicating that the residuals do not follow a normal distribution.\n",
    "\n",
    "Model 4 (All Data using indicator): This model may have residuals that appear closer to a normal distribution but could also show slight deviations, such as heavy tails or a slight skew.\n",
    "\n",
    "In summary:\n",
    "Plausible Normality: The model that best fits the assumption of normally distributed errors is the one whose residuals most closely match the symmetric, bell-shaped normal distribution curve (Model 1 or Model 4, depending on their actual forms).\n",
    "Non-Normality: There may be notable skewness, bimodality, or other anomalies in the other histograms (probably Models 2 and 3) that would imply the error terms depart from normalcy. The Simple Linear Regression assumption would be broken, possibly necessitating additional diagnostic testing or model modifications (such transformations).\n",
    "Examine closely how well the form of the histogram resembles the normal distribution overlay to determine the plausibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6bd3e9",
   "metadata": {},
   "source": [
    "13.\n",
    "\n",
    "(A) Test of Permutation\n",
    "The Permutation Test Explained:\n",
    "Hypothesis Null (H‚ÇÄ): Both the \"short\" and \"long\" wait periods have the same mean duration (i.e., ùúáshort = ùúálong Œºshort = Œº long).\n",
    "Method: In a permutation test, the labels of the two groups‚Äî\"short\" and \"long\" wait times‚Äîare switched, and the distribution of mean differences produced by the shuffling procedure is compared to the observed mean difference between these groups.\n",
    "Shuffling: Using this technique, the samples' \"kind\" label is randomly reassigned (without replacement), and the mean difference between the groups is then determined.\n",
    "\n",
    "Simulation: Measured data: Using the original labels, determine the mean difference between the two groups.\n",
    "Shuffling: Calculate the difference in means for each shuffled dataset after randomly rearranging the group labels (i.e., rearranging the 'kind' column).\n",
    "Again: Under the null hypothesis, build a distribution of mean differences by repeatedly doing the shuffling process (10,000 iterations, for example).\n",
    "P-value: Determine the p-value by contrasting the distribution of shuffled mean differences with the observed mean difference. The percentage of times the hypothesized mean difference is larger than or equal to the observed mean difference is known as the p-value.\n",
    "\n",
    "(B) Confidence Interval for Bootstrap\n",
    "The Bootstrap Confidence Interval explained:\n",
    "Method: Bootstrapping is the process of creating numerous new datasets that are the same size as the original dataset by sampling with replacement from each group (i.e., \"short\" and \"long\"). A mean difference between the two groups is computed for each bootstrap sample.\n",
    "The Resampling of Bootstrap: You can create variability in the sample means by simulating the process of sampling from the population by resampling with replacement within each group.\n",
    "\n",
    "Simulation: Measured data: Determine how much the \"long\" and \"short\" wait times differ on average.\n",
    "Bootstrapping: To generate a bootstrap sample, randomly sample (with replacement) the data points inside each category (\"short\" and \"long\").\n",
    "Again: To create a distribution of mean differences, repeat the resampling procedure numerous times (10,000 iterations, for example).\n",
    "Confidence Interval: By calculating the 2.5th and 97.5th percentiles of the distribution of mean differences, you can use the bootstrapped mean differences to get the 95% confidence interval.\n",
    "\n",
    "(a)\n",
    "How Sampling Methods Operate\n",
    "Permutation Test: This test simulates the null hypothesis, which states that there is no difference between the groups. Under the null hypothesis, an empirical distribution is produced by rearranging the group labels and computing the mean differences for each shuffle. The significance of the observed difference in means is then evaluated by comparing it with this distribution and the observed statistic.\n",
    "Confidence Interval for Bootstrap:\n",
    "By mimicking the procedure of population sampling, the bootstrap method operates. An empirical distribution of mean differences is created by repeatedly sampling each group with replacement and computing the mean differences. The variability in the mean differences is then represented by the relevant percentiles of this distribution, which is then used to generate the confidence interval.\n",
    "\n",
    "(b)\n",
    "Contrast with the Indicator Variable Method\n",
    "A binary indicator variable is utilized to describe the group membership (i.e., \"short\" vs. \"long\") and estimate the difference in means between these two groups in Question 11's indicator variable model, which is based on regression. This method treats the indicator variable as a predictor and applies a linear regression model.\n",
    "\n",
    "Comparables:\n",
    "The objective of the indicator variable approach, the bootstrap confidence interval, and the permutation test is to estimate the difference in means between the two groups.\n",
    "Empirical Methods: To evaluate the differences between the groups, all three approaches rely on empirical sampling techniques, including regression modelling, bootstrapping, and permutation.\n",
    "\n",
    "Disparities: Character of the Method:\n",
    "\n",
    "The permutation test evaluates the null hypothesis just using the data and randomization, without making any assumptions about a particular model.\n",
    "The bootstrap approach creates confidence intervals by resampling with replacement to evaluate the variability in the data.\n",
    "Regression models, which use statistical inference (such as hypothesis tests) based on model assumptions (e.g., normality of errors) and assume a linear relationship, are fitted via the indicator variable technique.\n",
    "\n",
    "Adaptability:\n",
    "\n",
    "Because they can be used in a wider range of scenarios and do not rely on rigid parametric assumptions (like the normalcy of errors), the permutation test and bootstrap approach are more adaptable.\n",
    "The indicator variable technique makes the assumptions that the model accurately depicts the connection between the variables and that the regression's residuals are normally distributed.\n",
    "\n",
    "Meaning:\n",
    "\n",
    "Based on the probability of witnessing the data under the null hypothesis, the permutation test yields a p-value.\n",
    "A range of reasonable mean difference values is provided by the bootstrap confidence interval.\n",
    "Hypothesis tests (such as t-tests) yield p-values for examining differences in group means, while the indicator variable approach generates parameter estimates with standard errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c810e",
   "metadata": {},
   "source": [
    "14.\n",
    "\n",
    "Reviewing the course wiki textbook has me a lot in terms of understanding the concept before tackling higher level concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ad23a",
   "metadata": {},
   "source": [
    "Summary:\n",
    "    \n",
    "### Summary of Everything Asked:\n",
    "\n",
    "#### 1. **Assessing Normality of Error Terms in Simple Linear Regression**:\n",
    "   - **Normality Assumption**: For Simple Linear Regression, error terms should be normally distributed (i.e., \\( \\epsilon_i \\sim N(0, \\sigma^2) \\)).\n",
    "   - **Histograms**: The plausibility of normality is checked by visualizing the distribution of residuals.\n",
    "     - **Symmetry and Bell-Shaped Curve**: A histogram that is symmetric around zero, resembling a bell-shaped curve, suggests normality.\n",
    "     - **Skewness/Bimodality**: If the histogram shows skewness or multiple peaks (bimodal), it suggests non-normality.\n",
    "   - **Conclusion**: The histogram that best matches the normal distribution curve supports the normality assumption. Histograms showing irregularities (skewness, bimodality) indicate violations of this assumption.\n",
    "\n",
    "#### 2. **Permutation Test for Two-Sample Comparison**:\n",
    "   - **Objective**: Compare the duration of events between \"short\" and \"long\" wait times, using a permutation test.\n",
    "   - **Null Hypothesis (H‚ÇÄ)**: There is no difference in duration between the short and long wait groups (\\( \\mu_{\\text{short}} = \\mu_{\\text{long}} \\)).\n",
    "   - **Procedure**:\n",
    "     - **Shuffling**: Randomly shuffle the group labels and compute the difference in means for each shuffled dataset.\n",
    "     - **Repeat**: Perform the shuffling many times (e.g., 10,000 iterations) to generate a distribution of mean differences.\n",
    "     - **P-value**: Compare the observed mean difference to the distribution of shuffled mean differences and calculate the p-value.\n",
    "\n",
    "#### 3. **Bootstrap Confidence Interval for Mean Difference**:\n",
    "   - **Objective**: Create a 95% confidence interval for the difference in means between the \"short\" and \"long\" groups using bootstrapping.\n",
    "   - **Procedure**:\n",
    "     - **Resampling**: Resample within each group with replacement to generate new samples.\n",
    "     - **Repeat**: Perform this resampling many times (e.g., 10,000 iterations) to get a distribution of mean differences.\n",
    "     - **Confidence Interval**: Calculate the 2.5th and 97.5th percentiles of the bootstrapped mean differences to get the 95% confidence interval.\n",
    "\n",
    "#### 4. **Explanation of Sampling Approaches**:\n",
    "   - **Permutation Test**: Tests the null hypothesis by reshuffling the group labels and comparing the observed difference to the shuffled data.\n",
    "   - **Bootstrap Confidence Interval**: Uses resampling with replacement to generate variability in sample means and estimate a confidence interval.\n",
    "   \n",
    "#### 5. **Comparison with Indicator Variable Model Approach**:\n",
    "   - **Similarities**:\n",
    "     - All methods (permutation test, bootstrap, and indicator variable model) aim to estimate the difference in means between groups.\n",
    "     - They are empirical methods used to assess the differences in a data-driven manner.\n",
    "   - **Differences**:\n",
    "     1. **Nature**:\n",
    "        - **Permutation test** and **bootstrap method** are **non-parametric** (do not require assumptions about data distribution).\n",
    "        - **Indicator variable approach** is **parametric** (assumes linearity and normality of residuals).\n",
    "     2. **Flexibility**:\n",
    "        - **Permutation and bootstrap** are more flexible since they make fewer assumptions.\n",
    "        - **Indicator variable approach** assumes a specific model and error distribution.\n",
    "     3. **Interpretation**:\n",
    "        - **Permutation test** gives a p-value.\n",
    "        - **Bootstrap** provides a confidence interval.\n",
    "        - **Indicator variable approach** estimates mean differences with statistical significance tests (e.g., t-test).\n",
    "\n",
    "### Conclusion:\n",
    "- **Permutation Test and Bootstrap** are flexible, non-parametric methods that can be used when assumptions like normality might not hold.\n",
    "- The **Indicator Variable Regression** approach is a **parametric** method that assumes normality of residuals and linear relationships but provides statistical tests to evaluate group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca5e2e",
   "metadata": {},
   "source": [
    "Link: https://chatgpt.com/c/672bd8e2-3f0c-800e-be85-67803b29197b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950035fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
